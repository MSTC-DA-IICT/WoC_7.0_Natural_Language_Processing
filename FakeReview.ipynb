{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: nltk in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: flask in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (3.0.2)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from flask) (3.0.2)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from flask) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from flask) (2.1.2)\n",
      "Requirement already satisfied: click>=8.1.3 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from flask) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from flask) (1.7.0)\n",
      "Requirement already satisfied: colorama in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from click>=8.1.3->flask) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from Jinja2>=3.1.2->flask) (2.1.5)\n",
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.11.1-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting Twisted>=18.9.0 (from scrapy)\n",
      "  Downloading twisted-24.3.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting cryptography>=36.0.0 (from scrapy)\n",
      "  Downloading cryptography-42.0.5-cp39-abi3-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cssselect>=0.9.1 (from scrapy)\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting itemloaders>=1.0.1 (from scrapy)\n",
      "  Downloading itemloaders-1.1.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting parsel>=1.5.0 (from scrapy)\n",
      "  Downloading parsel-1.9.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pyOpenSSL>=21.0.0 (from scrapy)\n",
      "  Downloading pyOpenSSL-24.1.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting queuelib>=1.4.2 (from scrapy)\n",
      "  Downloading queuelib-1.6.2-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting service-identity>=18.1.0 (from scrapy)\n",
      "  Downloading service_identity-24.1.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting w3lib>=1.17.0 (from scrapy)\n",
      "  Downloading w3lib-2.1.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting zope.interface>=5.1.0 (from scrapy)\n",
      "  Downloading zope.interface-6.2-cp312-cp312-win_amd64.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.1 kB ? eta -:--:--\n",
      "     ---------------------------- ----------- 30.7/43.1 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 43.1/43.1 kB 521.0 kB/s eta 0:00:00\n",
      "Collecting protego>=0.1.15 (from scrapy)\n",
      "  Downloading Protego-0.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting itemadapter>=0.1.0 (from scrapy)\n",
      "  Downloading itemadapter-0.8.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: setuptools in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from scrapy) (69.2.0)\n",
      "Requirement already satisfied: packaging in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from scrapy) (24.0)\n",
      "Collecting tldextract (from scrapy)\n",
      "  Downloading tldextract-5.1.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting lxml>=4.4.1 (from scrapy)\n",
      "  Downloading lxml-5.2.1-cp312-cp312-win_amd64.whl.metadata (3.5 kB)\n",
      "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
      "  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting cffi>=1.12 (from cryptography>=36.0.0->scrapy)\n",
      "  Downloading cffi-1.16.0-cp312-cp312-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: attrs>=19.1.0 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (23.2.0)\n",
      "Collecting pyasn1 (from service-identity>=18.1.0->scrapy)\n",
      "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting pyasn1-modules (from service-identity>=18.1.0->scrapy)\n",
      "  Downloading pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting automat>=0.8.0 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading Automat-22.10.0-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting constantly>=15.1 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting hyperlink>=17.1.1 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting incremental>=22.10.0 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading incremental-22.10.0-py2.py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting twisted-iocpsupport<2,>=1.0.2 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading twisted_iocpsupport-1.0.4-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (4.10.0)\n",
      "Requirement already satisfied: idna in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from tldextract->scrapy) (3.6)\n",
      "Requirement already satisfied: requests>=2.1.0 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from tldextract->scrapy) (2.31.0)\n",
      "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
      "  Downloading requests_file-2.0.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting filelock>=3.0.8 (from tldextract->scrapy)\n",
      "  Downloading filelock-3.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: six in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from automat>=0.8.0->Twisted>=18.9.0->scrapy) (1.16.0)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=36.0.0->scrapy)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2024.2.2)\n",
      "Downloading Scrapy-2.11.1-py2.py3-none-any.whl (287 kB)\n",
      "   ---------------------------------------- 0.0/287.8 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 61.4/287.8 kB 1.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 143.4/287.8 kB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 225.3/287.8 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 287.8/287.8 kB 1.6 MB/s eta 0:00:00\n",
      "Downloading cryptography-42.0.5-cp39-abi3-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.2/2.9 MB 5.0 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.3/2.9 MB 3.5 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.4/2.9 MB 3.3 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.5/2.9 MB 3.2 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.7/2.9 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.9/2.9 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.1/2.9 MB 3.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.3/2.9 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.4/2.9 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.6/2.9 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.9/2.9 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.2/2.9 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.5/2.9 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.6/2.9 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.7/2.9 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 3.9 MB/s eta 0:00:00\n",
      "Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading itemadapter-0.8.0-py3-none-any.whl (11 kB)\n",
      "Downloading itemloaders-1.1.0-py3-none-any.whl (11 kB)\n",
      "Downloading lxml-5.2.1-cp312-cp312-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.5/3.8 MB 9.6 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.8/3.8 MB 8.9 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.5/3.8 MB 8.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.0/3.8 MB 8.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 2.4/3.8 MB 9.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.0/3.8 MB 8.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.6/3.8 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 7.6 MB/s eta 0:00:00\n",
      "Downloading parsel-1.9.0-py2.py3-none-any.whl (17 kB)\n",
      "Downloading Protego-0.3.1-py2.py3-none-any.whl (8.5 kB)\n",
      "Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
      "Downloading pyOpenSSL-24.1.0-py3-none-any.whl (56 kB)\n",
      "   ---------------------------------------- 0.0/56.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 56.9/56.9 kB 1.5 MB/s eta 0:00:00\n",
      "Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
      "Downloading service_identity-24.1.0-py3-none-any.whl (12 kB)\n",
      "Downloading twisted-24.3.0-py3-none-any.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.4/3.2 MB 13.9 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.9/3.2 MB 8.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 7.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.6/3.2 MB 6.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.7/3.2 MB 6.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.7/3.2 MB 6.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.7/3.2 MB 6.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.7/3.2 MB 6.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.2/3.2 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.2/3.2 MB 5.5 MB/s eta 0:00:00\n",
      "Downloading w3lib-2.1.2-py3-none-any.whl (21 kB)\n",
      "Downloading zope.interface-6.2-cp312-cp312-win_amd64.whl (204 kB)\n",
      "   ---------------------------------------- 0.0/204.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 204.5/204.5 kB 6.1 MB/s eta 0:00:00\n",
      "Downloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.6/97.6 kB 5.5 MB/s eta 0:00:00\n",
      "Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
      "Downloading cffi-1.16.0-cp312-cp312-win_amd64.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/182.0 kB ? eta -:--:--\n",
      "   ------------------------------------- - 174.1/182.0 kB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 182.0/182.0 kB 3.7 MB/s eta 0:00:00\n",
      "Downloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
      "Downloading filelock-3.13.3-py3-none-any.whl (11 kB)\n",
      "Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "   ---------------------------------------- 0.0/74.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 74.6/74.6 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading requests_file-2.0.0-py2.py3-none-any.whl (4.2 kB)\n",
      "Downloading twisted_iocpsupport-1.0.4-cp312-cp312-win_amd64.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/47.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 47.6/47.6 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "   ---------------------------------------- 0.0/85.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 85.3/85.3 kB 4.7 MB/s eta 0:00:00\n",
      "Downloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/181.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 181.2/181.2 kB 5.5 MB/s eta 0:00:00\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "   ---------------------------------------- 0.0/117.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 117.6/117.6 kB 3.5 MB/s eta 0:00:00\n",
      "Installing collected packages: twisted-iocpsupport, PyDispatcher, incremental, zope.interface, w3lib, queuelib, pycparser, pyasn1, protego, lxml, jmespath, itemadapter, hyperlink, filelock, cssselect, constantly, automat, Twisted, requests-file, pyasn1-modules, parsel, cffi, tldextract, itemloaders, cryptography, service-identity, pyOpenSSL, scrapy\n",
      "Successfully installed PyDispatcher-2.0.7 Twisted-24.3.0 automat-22.10.0 cffi-1.16.0 constantly-23.10.4 cryptography-42.0.5 cssselect-1.2.0 filelock-3.13.3 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.8.0 itemloaders-1.1.0 jmespath-1.0.1 lxml-5.2.1 parsel-1.9.0 protego-0.3.1 pyOpenSSL-24.1.0 pyasn1-0.6.0 pyasn1-modules-0.4.0 pycparser-2.22 queuelib-1.6.2 requests-file-2.0.0 scrapy-2.11.1 service-identity-24.1.0 tldextract-5.1.2 twisted-iocpsupport-1.0.4 w3lib-2.1.2 zope.interface-6.2\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install nltk\n",
    "!pip install flask\n",
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import string\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             category  rating label  \\\n",
       "0  Home_and_Kitchen_5     5.0    CG   \n",
       "1  Home_and_Kitchen_5     5.0    CG   \n",
       "2  Home_and_Kitchen_5     5.0    CG   \n",
       "3  Home_and_Kitchen_5     1.0    CG   \n",
       "4  Home_and_Kitchen_5     5.0    CG   \n",
       "\n",
       "                                               text_  \n",
       "0  Love this!  Well made, sturdy, and very comfor...  \n",
       "1  love it, a great upgrade from the original.  I...  \n",
       "2  This pillow saved my back. I love the look and...  \n",
       "3  Missing information on how to use it, but it i...  \n",
       "4  Very nice set. Good quality. We have had the s...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('fakeReviewData.csv') \n",
    "dataframe.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WEAK ON CURRENT SCIENCE.\\nAfter seeing it twice, I agree with much (but not all) of the positive five star reviews. Out of respect for those who READ reviews, I\\'ll not repeat everything that I like about the presentation. I found the goofy oversize earrings, hairdo, and facial hair arrangement of Daniel Vitalis, (described as a \"Wild Food Expert\") distracting. UGH. Ditto for David Wolfe, who had an extremely goofy wild hairdo. On the other hand, Jon Gabriel, described as an \"author and weight loss expert\" was nicely groomed and a good presenter. His story of personal transformation of a fellow of over 400 pounds (whew) to becoming a jock of normal weight was inspiring. Christiane Northrup preserves her rank as one of America\\'s cutest doctors. A really nice looking woman! Presentations by Dr. Mercola, Jason Vale, Kris Carr, Alejandro Junger were fine. It was disappointing to have Jamie Oliver (so popular in the UK) give Baby Cow Growth Fluid a pass with unscientific but popular ideas on milk. None of the presenters had anything (ZILCH) to say about the work of Doctor T. Colin Campbell. Milk does a body bad. It was good to see all presenters take a stand against sugar. They all agreed on the evils of sugar and refined carbohydrates. With all respect to Dr. Northrup, her \"It\\'s not fat that makes you fat - it\\'s sugar\" statement will not pass muster in the community of experts. Recognizing the evils of sugar should NOT be mutually exclusive to recognizing the proven dangers of fat - particularly fat from dead animals - and EXTRACTED fat of ALL kinds. Olive Oil is NOT a health food.\\n\\n<a data-hook=\"product-link-linked\" class=\"a-link-normal\" href=\"/The-China-Study-The-Most-Comprehensive-Study-of-Nutrition-Ever-Conducted-And-the-Startling-Implications-for-Diet-Weight-Loss-And-Long-term-Health/dp/1932100660/ref=cm_cr_arp_d_rvw_txt?ie=UTF8\">The China Study: The Most Comprehensive Study of Nutrition Ever Conducted And the Startling Implications for Diet, Weight Loss, And Long-term Health</a>\\n\\n<a data-hook=\"product-link-linked\" class=\"a-link-normal\" href=\"/Forks-Over-Knives/dp/B0053ZHZI2/ref=cm_cr_arp_d_rvw_txt?ie=UTF8\">Forks Over Knives</a>\\n\\n<a data-hook=\"product-link-linked\" class=\"a-link-normal\" href=\"/Prevent-and-Reverse-Heart-Disease-The-Revolutionary-Scientifically-Proven-Nutrition-Based-Cure/dp/1583333002/ref=cm_cr_arp_d_rvw_txt?ie=UTF8\">Prevent and Reverse Heart Disease: The Revolutionary, Scientifically Proven, Nutrition-Based Cure</a>\\n\\n<a data-hook=\"product-link-linked\" class=\"a-link-normal\" href=\"/The-Plant-Based-Journey-A-Step-by-Step-Guide-for-Transitioning-to-a-Healthy-Lifestyle-and-Achieving-Your-Ideal-Weight/dp/1941631363/ref=cm_cr_arp_d_rvw_txt?ie=UTF8\">The Plant-Based Journey: A Step-by-Step Guide for Transitioning to a Healthy Lifestyle and Achieving Your Ideal Weight</a>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.dropna(inplace=True) \n",
    "dataframe['length'] = dataframe['text_'].apply(len) \n",
    "dataframe[dataframe['label']=='OR'][['text_','length']].sort_values(by='length',ascending=False).head().iloc[0].text_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertmyTxt(rv): \n",
    "    np = [c for c in rv if c not in string.punctuation] \n",
    "    np = ''.join(np) \n",
    "    return [w for w in np.split() if w.lower() not in stopwords.words('english')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(dataframe['text_'],dataframe['label'],test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=convertmyTxt)),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('classifier',RandomForestClassifier())\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only run the first time\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['random_forest_model.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip.fit(x_train,y_train) \n",
    "filename = 'random_forest_model.pkl'\n",
    "joblib.dump(pip, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OR' 'OR' 'OR' ... 'CG' 'OR' 'CG']\n"
     ]
    }
   ],
   "source": [
    "randomForestClassifier = pip.predict(x_test) \n",
    "print(randomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:  85.65%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of the model: ',str(np.round(accuracy_score(y_test,randomForestClassifier)*100,2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=convertmyTxt)),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('classifier',SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['support_vector_classifier.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip.fit(x_train,y_train)\n",
    "filename = 'support_vector_classifier.pkl'\n",
    "joblib.dump(pip, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['OR', 'CG', 'OR', ..., 'CG', 'CG', 'OR'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supportVectorClassifier = pip.predict(x_test)\n",
    "supportVectorClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the model: 90.34%\n"
     ]
    }
   ],
   "source": [
    "print('accuracy of the model:',str(np.round(accuracy_score(y_test,supportVectorClassifier)*100,2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=convertmyTxt)),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('classifier',LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logisticRegression.pkl']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip.fit(x_train,y_train)\n",
    "filename = 'logisticRegression.pkl'\n",
    "joblib.dump(pip, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['OR', 'CG', 'OR', ..., 'CG', 'CG', 'OR'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegression = pip.predict(x_test)#here we are predicting the accuracy of the Random Forest Classifier model\n",
    "logisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the model: 88.97%\n"
     ]
    }
   ],
   "source": [
    "print('accuracy of the model:',str(np.round(accuracy_score(y_test,logisticRegression)*100,2)) + '%')#here we are predicting the accuracy of the Random Forest Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = pip.predict([\"This is an awsome product\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['OR'], dtype=object)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load(\".\\\\models\\\\randomForest.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['OR'], dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = loaded_model.predict([\"This is an awsome product\"])\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load(\".\\\\models\\\\SVC.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting html5lib\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: six>=1.9 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from html5lib) (1.16.0)\n",
      "Collecting webencodings (from html5lib)\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "   ---------------------------------------- 0.0/112.2 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 30.7/112.2 kB 1.3 MB/s eta 0:00:01\n",
      "   --------------------- ----------------- 61.4/112.2 kB 812.7 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 102.4/112.2 kB 837.8 kB/s eta 0:00:01\n",
      "   -------------------------------------- 112.2/112.2 kB 815.5 kB/s eta 0:00:00\n",
      "Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: webencodings, html5lib\n",
      "Successfully installed html5lib-1.1 webencodings-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['OR'], dtype=object)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = loaded_model.predict([\"This is an awsome product\"])\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load(\".\\\\models\\\\logisticRegression.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['OR', 'OR'], dtype=object)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = loaded_model.predict([\"This is an awsome product\",\"Pesa vasul ðŸ‘Œ\"])\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "class AmazonReviewsSpider(scrapy.Spider):\n",
    "    name = \"amazon_reviews\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        asin_list = ['B09G9FPHY6']\n",
    "        for asin in asin_list:\n",
    "            amazon_reviews_url = f'https://www.amazon.com/product-reviews/{asin}/'\n",
    "            yield scrapy.Request(url=amazon_reviews_url, callback=self.parse_reviews, meta={'asin': asin})\n",
    "\n",
    "    def parse_reviews(self, response):\n",
    "        asin = response.meta['asin']\n",
    "        \n",
    "        ## Parse Product Reviews\n",
    "        review_elements = response.css(\"#cm_cr-review_list div.review\")\n",
    "        for review_element in review_elements:\n",
    "            yield {\n",
    "                    \"asin\": asin,\n",
    "                    \"text\": \"\".join(review_element.css(\"span[data-hook=review-body] ::text\").getall()).strip(),\n",
    "                    \"title\": review_element.css(\"*[data-hook=review-title]>span::text\").get(),\n",
    "                    \"location_and_date\": review_element.css(\"span[data-hook=review-date] ::text\").get(),\n",
    "                    \"verified\": bool(review_element.css(\"span[data-hook=avp-badge] ::text\").get()),\n",
    "                    \"rating\": review_element.css(\"*[data-hook*=review-star-rating] ::text\").re(r\"(\\d+\\.*\\d*) out\")[0],\n",
    "                    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-05 17:54:08 [scrapy.utils.log] INFO: Scrapy 2.11.1 started (bot: scrapybot)\n",
      "2024-04-05 17:54:08 [scrapy.utils.log] INFO: Versions: lxml 5.2.1.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.9.0, w3lib 2.1.2, Twisted 24.3.0, Python 3.12.2 (tags/v3.12.2:6abddd9, Feb  6 2024, 21:26:36) [MSC v.1937 64 bit (AMD64)], pyOpenSSL 24.1.0 (OpenSSL 3.2.1 30 Jan 2024), cryptography 42.0.5, Platform Windows-11-10.0.22621-SP0\n",
      "Usage\n",
      "=====\n",
      "  scrapy runspider [options] <spider_file>\n",
      "runspider: error: File not found: amazon_reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!scrapy runspider amazon_reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-05 18:44:21 [scrapy.utils.log] INFO: Scrapy 2.11.1 started (bot: scrapybot)\n",
      "2024-04-05 18:44:22 [scrapy.utils.log] INFO: Versions: lxml 5.2.1.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.9.0, w3lib 2.1.2, Twisted 24.3.0, Python 3.12.2 (tags/v3.12.2:6abddd9, Feb  6 2024, 21:26:36) [MSC v.1937 64 bit (AMD64)], pyOpenSSL 24.1.0 (OpenSSL 3.2.1 30 Jan 2024), cryptography 42.0.5, Platform Windows-11-10.0.22621-SP0\n",
      "2024-04-05 18:44:22 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-04-05 18:44:22 [py.warnings] WARNING: D:\\Buijness\\ML\\NLP\\FakeReviewDetection\\FakeReview\\Lib\\site-packages\\scrapy\\utils\\request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2024-04-05 18:44:22 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2024-04-05 18:44:22 [scrapy.extensions.telnet] INFO: Telnet Password: 02d4df9568474026\n",
      "2024-04-05 18:44:22 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-04-05 18:44:22 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'RETRY_HTTP_CODES': [503], 'RETRY_TIMES': 3, 'SPIDER_LOADER_WARN_ONLY': True}\n",
      "2024-04-05 18:44:22 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-04-05 18:44:22 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-04-05 18:44:22 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-04-05 18:44:22 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-04-05 18:44:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-04-05 18:44:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2024-04-05 18:44:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.amazon.com/product-reviews/B0744NSM3N/> (failed 1 times): 503 Service Unavailable\n",
      "2024-04-05 18:44:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.amazon.com/product-reviews/B0744NSM3N/> (failed 2 times): 503 Service Unavailable\n",
      "2024-04-05 18:44:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.amazon.com/product-reviews/B0744NSM3N/> (failed 3 times): 503 Service Unavailable\n",
      "2024-04-05 18:44:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.amazon.com/product-reviews/B0744NSM3N/> (failed 4 times): 503 Service Unavailable\n",
      "2024-04-05 18:44:23 [scrapy.core.engine] DEBUG: Crawled (503) <GET https://www.amazon.com/product-reviews/B0744NSM3N/> (referer: None)\n",
      "2024-04-05 18:44:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <503 https://www.amazon.com/product-reviews/B0744NSM3N/>: HTTP status code is not handled or not allowed\n",
      "2024-04-05 18:44:23 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2024-04-05 18:44:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 968,\n",
      " 'downloader/request_count': 4,\n",
      " 'downloader/request_method_count/GET': 4,\n",
      " 'downloader/response_bytes': 10346,\n",
      " 'downloader/response_count': 4,\n",
      " 'downloader/response_status_count/503': 4,\n",
      " 'elapsed_time_seconds': 1.142516,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2024, 4, 5, 13, 14, 23, 508827, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 26532,\n",
      " 'httpcompression/response_count': 4,\n",
      " 'httperror/response_ignored_count': 1,\n",
      " 'httperror/response_ignored_status_count/503': 1,\n",
      " 'log_count/DEBUG': 5,\n",
      " 'log_count/ERROR': 1,\n",
      " 'log_count/INFO': 11,\n",
      " 'log_count/WARNING': 1,\n",
      " 'response_received_count': 1,\n",
      " 'retry/count': 3,\n",
      " 'retry/max_reached': 1,\n",
      " 'retry/reason_count/503 Service Unavailable': 3,\n",
      " 'scheduler/dequeued': 4,\n",
      " 'scheduler/dequeued/memory': 4,\n",
      " 'scheduler/enqueued': 4,\n",
      " 'scheduler/enqueued/memory': 4,\n",
      " 'start_time': datetime.datetime(2024, 4, 5, 13, 14, 22, 366311, tzinfo=datetime.timezone.utc)}\n",
      "2024-04-05 18:44:23 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!scrapy runspider amazonScrapper.py -a asin_list=\"B0744NSM3N\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml>=4.6.0 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (5.2.1)\n",
      "Requirement already satisfied: requests_html>=0.10.0 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (0.10.0)\n",
      "Requirement already satisfied: requests in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from requests_html>=0.10.0) (2.31.0)\n",
      "Requirement already satisfied: pyquery in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from requests_html>=0.10.0) (2.0.0)\n",
      "Requirement already satisfied: fake-useragent in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from requests_html>=0.10.0) (1.5.1)\n",
      "Requirement already satisfied: parse in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from requests_html>=0.10.0) (1.20.1)\n",
      "Requirement already satisfied: bs4 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from requests_html>=0.10.0) (0.0.2)\n",
      "Requirement already satisfied: w3lib in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from requests_html>=0.10.0) (2.1.2)\n",
      "Requirement already satisfied: pyppeteer>=0.0.14 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from requests_html>=0.10.0) (2.0.0)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from pyppeteer>=0.0.14->requests_html>=0.10.0) (1.4.4)\n",
      "Requirement already satisfied: certifi>=2023 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from pyppeteer>=0.0.14->requests_html>=0.10.0) (2024.2.2)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from pyppeteer>=0.0.14->requests_html>=0.10.0) (7.1.0)\n",
      "Requirement already satisfied: pyee<12.0.0,>=11.0.0 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from pyppeteer>=0.0.14->requests_html>=0.10.0) (11.1.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from pyppeteer>=0.0.14->requests_html>=0.10.0) (4.66.2)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from pyppeteer>=0.0.14->requests_html>=0.10.0) (1.26.18)\n",
      "Requirement already satisfied: websockets<11.0,>=10.0 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from pyppeteer>=0.0.14->requests_html>=0.10.0) (10.4)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from bs4->requests_html>=0.10.0) (4.12.3)\n",
      "Requirement already satisfied: cssselect>=1.2.0 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from pyquery->requests_html>=0.10.0) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from requests->requests_html>=0.10.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from requests->requests_html>=0.10.0) (3.6)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests_html>=0.10.0) (3.18.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests_html>=0.10.0) (4.10.0)\n",
      "Requirement already satisfied: colorama in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from tqdm<5.0.0,>=4.42.1->pyppeteer>=0.0.14->requests_html>=0.10.0) (0.4.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\buijness\\ml\\nlp\\fakereviewdetection\\fakereview\\lib\\site-packages (from beautifulsoup4->bs4->requests_html>=0.10.0) (2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"lxml>=4.6.0\" \"requests_html>=0.10.0\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FakeReview",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
